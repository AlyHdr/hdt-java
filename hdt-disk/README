
Motivation
============

Generating HDT Files can take a huge amount of intermediate memory to manage all the long RDF strings. This package uses disk-based key-value stores to keep this intermediate information, therefore allowing the generation of bigger HDT files.

If you have enough memory to generate an HDT file, you should use the memory-based implementation, it will possibly faster.


Compiling
=============

Just run "ant jar" under hdt-disk to compile the disk-based implementations. This will generate the hdt-disk.jar.


Executing
============

To convert using the disk-based implementations, you must add the hdt-lib.jar and hdt-disk.jar to the classpath together with the needed dependencies (KyotoCabinet, JDBM3, BerkeleyDB). By default, the rdf2hdt.sh/.bat launch script includes all these dependencies.

You will need to specify the implementation that you want as an option to the RDF2HDT program, for example by using a config file

./rdf2hdt -config presets/jdbm.hdtcfg source.nt dest.hdt


Options of the Config File
============

You may indicate an implementation both for Dictionary and Triples. The most important is the Dictionary because it contains very long strings, the triples are usually safe to be kept in memory.

Parameters:
	- "tempDictionary.type":  (hash|jdbm|berkeley|kyoto)
	- "tempDictionary.cache": Amount of data to be kept cached in memory, specified as: <number><|K|k|M|m|G|g> 

	- "tempTriples.type":  (hash|jdbm|berkeley)
	- "tempTriples.cache": Amount of data to be kept cached in memory, specified as: <number><|K|k|M|m|G|g> 

The library ends up with the generated HDT in memory. Therefore, if you want to generate an HDT of 1Gb, You could specify -Xmx1500M and a tempDictionary.cache of 300M to have enough space.
